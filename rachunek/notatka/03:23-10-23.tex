\section{23.10.23 : Interpretacje geometryczne WWO}

Rozważmy zmienną losową $X$ taką, że $\expected{X^2}<\infty$. Interesuje nas zagadnienie regresji, mianowicie obserwując inną zmienną losową $Z$ chcemy móc $X$ aproksymować. Przez przybliżanie $X$ rozumiemy przybliżanie średniokwadratowe. 

Szukamy więc funkcji mierzalnej $h_0:\R\to\R$ takiej, żeby 
$$\expected{(X-h_0(Z))^2}=\inf_{h:\R\to\R}\expected{(X-h(Z))^2}$$

\begin{fact}\label{fakt 3.1}
  Dla każdej zmiennej losowej $Y$ mierzalnej względem $\sigma(Z)$ (co w skrócie będziemy notować $Y\in\sigma(Z)$) istnieje $h$ takie, że $Y=h(Z)$.
\end{fact}

\begin{proof}
  Zadanie, moja próba poniżej.

  Zaczynamy od $Y$ będącego funkcją prostą i przechodzimy do coraz to bardziej skomplikowanych postaci funkcji mierzalnych.

  Jeżeli $Y=\mathds{1}_A$, to ponieważ $Y$ jest $\sigma(Z)$-mierzalne, mamy $A\in\sigma(Z)$. To znaczy, że istnieje $B\in Bor(\R)$ taki, że $Z(A)=B$, czyli $A=Z^{-1}[B]$ i wówczas
  $$Y=\mathds{1}_A=\mathds{1}_{Z^{-1}[B]}=\mathds{1}_B(Z)$$

  Zrobimy tutaj jeszcze krok $Y=\sum a_i\mathds{1}_{A_i}$. Dla każdego $i$ wiemy, że $\mathds{1}_{A_i}=h_i(Z)$, gdyż są to funkcje $\sigma(Z)$-mierzalne. W takim razie:
  $$Y=\sum a_i\mathds{1}_{A_i}=\sum a_i\cdot h_i(Z)=\left[\sum a_i\cdot h_i\right](Z)$$
  a więc szukane $h=\sum a_i\cdot h_i$.

  Teraz załóżmy, że istnieje ciąg funkcji prostych $s_1\leq s_2\leq...\leq Y$ taki, że $Y=\lim s_i$. Wówczas pokazaliśmy już, że każda funkcja $s_i=h_i(Z)$ dla pewnego borelowskiego $h_i:\R\to\R$. Ciąg $s_i$ jest zbieżny, więc również ciąg $h_i$ musi zbiegać do pewnego $h$. Wówczas dla dowolnego $\omega\in\Omega$
  $$Y(\omega)=\lim s_i(\omega)=\lim h_i(Z(\omega))=h(Z(\omega))$$
  czyli $Y=h(Z)$ dla $h=\lim h_i$.

  Dla formalności należy rozważyć jeszcze $Y=Y^+-Y^-$, gdzie $Y^+$ oraz $Y^-$ podlegają pod poprzedni podpunkt.
\end{proof}

$$\expected{(X-h(Z))^2}=\inf_{Y\in\sigma(Z)}\expected{(X-Y)^2}$$
mając pewną wiedzę o przestrzeniach Hilberta jest do tego dość naturalne podejście: rzut ortogonalny.

Dla $\sigma$-ciała $\set{G}\subseteq\set{F}$ będziemy rozważać
$$L^2(\set{G})=\{Y\in\set{G}\;:\;\expected{Y^2}<\infty\}$$
wówczas $L^2(\set{G})$ jest przestrzenią Euklidesową z iloczynem skalarnym $\langle X, Y\rangle=\expected{XY}$, który z kolei zadaje normę
$$\|X\|=\sqrt{\expected{X^2}}.$$

Używając tego języka będziemy wiedzieli jak szukać $Y$ z początku wykładu, ale najpierw fakt pomocniczy do twierdzenia które nadejdzie lada moment.

\begin{fact}[warunkowa wersja nierówności Cauchy'ego-Schwartza]\label{warunkowy Cauchy-szwarc}
  Dla zmiennych  $X,Y$ takich, że $\expected{X^2},\expected{Y^2}<\infty$ i $\sigma$-ciała $\set{G}\subseteq\set{F}$ zachodzi
  $$\expected{|XY|}{\set{G}}\leq\expected{X^2}{\set{G}}^{\frac{1}{2}}\expected{Y^2}{\set{G}}^{\frac{1}{2}}$$
\end{fact}

\begin{proof}
  Zadanie, tutaj moje podejście.


  Zauważmy na początku, że
  $$XY=\underbrace{\frac{(\expected{Y^2}{\set{G}}+\frac{1}{n})^{1/4}}{(\expected{X^2}{\set{G}}+\frac{1}{n})^{1/4}}X}_{X_n}\underbrace{\cdot\frac{(\expected{X^2}{\set{G}}+\frac{1}{n})^{1/4}}{(\expected{Y^2}{\set{G}}+\frac{1}{n})^{1/4}}Y}_{Y_n}$$
  przy czym korzystamy z $\frac{1}{n}$, żeby na pewno nie dzielić przez $0$ gdy np. $\expected{X^2}{\set{G}}=0$.

  Dalej zauważmy, że ponieważ $(X_n-Y_n)^2\geq0$, to również
  $$0\leq\expected{(X_n-Y_n)^2}{\set{G}}=\expected{X_n^2+Y_n^2-2X_nY_n}{\set{G}}$$
  czyli korzystając z liniowości i przenosząc $\expected{X_nY_n}{\set{G}}$ na lewą stronę nierówności dostajemy
  \begin{align*}\expected{XY}{\set{G}}&={\color{blue}
    \expected{X_nY_n}{\set{G}}\leq 
  \frac{1}{2}\expected{X_n^2}{\set{G}}+\frac{1}{2}\expected{Y_n^2}{\set{G}}}=\\ 
  &=\frac{1}{2}\expected{\frac{(\expected{Y^2}{\set{G}}+\frac{1}{n})^{1/2}}{(\expected{X^2}{\set{G}}+\frac{1}{n})^{1/2}}X^2}{\set{G}}+\frac{1}{2}\expected{\frac{(\expected{X^2}{\set{G}}+\frac{1}{n})^{1/2}}{(\expected{Y^2}{\set{G}}+\frac{1}{n})^{1/2}}Y^2}{\set{G}}=(\heartsuit)
  \end{align*}
  a ponieważ $\frac{(\expected{Y^2}{\set{G}}+\frac{1}{n})^{1/2}}{(\expected{X^2}{\set{G}}+\frac{1}{n})^{1/2}}$ jest $\set{G}$-mierzalne, to możemy wyciągnąć je przed nawias:
  \begin{align*}
    (\heartsuit)&=\frac{1}{2}\cdot \frac{ (\expected{Y^2}{\set{G}}+\frac{1}{n})^{1/2}}{ (\expected{X^2}{\set{G}}+\frac{1}{n})^{1/2}} \expected{X^2}{\set{G}}+\frac{1}{2}\cdot  \frac{ (\expected{X^2}{\set{G}}+\frac{1}{n})^{1/2}}{ (\expected{Y^2}{\set{G}}+\frac{1}{n})^{1/2}} \expected{Y^2}{\set{G}}\to\\ 
                &\xrightarrow{n\to\infty}(\expected{Y^2}{\set{G}})^{1/2}(\expected{X^2}{\set{G}})^{1/2}
  \end{align*}
\end{proof}

Z tego nierówności w fakcie \ref{warunkowy Cauchy-szwarc} wynika, że dla $Y=1$ mamy
$$\expected{|X|}{\set{G}}^2\leq\expected{X^2}{\set{G}}\implies\expected{\expected{X}{Y}^2}<\infty$$

\begin{theorem}[wwo jest rzutem ortogonalnym na $L^2(\set{G})$]
  Niech $X$ będzie zmienną losową taką, że $\expected{X^2}<\infty$, a $\set{G}\subseteq\set{F}$ jest $\sigma$-ciałem. Wówczas 
  $$\expected{X}{\set{G}}\in L^2(\set{G})$$
  jest \acc{rzutem ortogonalnym} $X$ na $L^2(\set{G})$.
\end{theorem}

\begin{center}\begin{tikzpicture}
  \node (X) at (0, 3) {$X$};
  \filldraw (0, 2.7) circle (1.5pt);

  \filldraw (-2, 0) circle (1.5pt);
  \node (0) at (-2, -0.3) {$0$};
  
  \filldraw (0, 0);
  \draw(-2, 0)--(0, 2.7);
  \draw[dashed](0, 2.7)--(0, 0);

  %\draw (-3, 1.5) rectangle (3, -1.5);
  %\draw plot[smooth, tension=2] coordinates {(-3, -2) (3, -1.5)};
  \draw (-3, 1.5) to[out=-20,in=220] (3, 2);
  \draw (3, 2) to[out=5, in=5] (3, -2);
  \draw (3, -2) to[out=150, in=20] (-3, -1.5);
  \draw (-3, -1.5) to[out=170, in=200] (-3, 1.5);

  \node at (2.7, -1) {$L^2(\set{G})$};
\end{tikzpicture}\end{center}

Dokładniej, $\expected{X}{\set{G}}$ daje minimum zbioru $\{\expected{(X-Y)^2}\;:\;Y\in L^2(\set{G})\}$. Z faktu \ref{fakt 3.1} dla $\set{G}=\sigma(Z)$, $\expected{X}{\set{G}}=h_0(Z)$ dla pewnego $h_0$.

\begin{proof}
  Dla $Y\in L^2(\set{G})$ mamy 
  \begin{align*}
    \expected{(X-Y)^2}&=\expected{((X-\expected{X}{\set{G}})-\overbrace{(Y-\expected{X}{\set{G}})}^{=Y'})^2}=\\
                      &=\expected{(X-\expected{X}{\set{G}})^2}-2\expected{(X-\expected{X}{\set{G}})Y'}+\expected{(Y')^2}=\\
                      &\overset{\star}{=}\expected{(X-\expected{X}{\set{G}})^2}+\expected{(Y')^2}
  \end{align*}
  Zauważmy, że
  \begin{align*}\expected{Y'X}{\set{G}}&=Y'\expected{X}{\set{G}}\\
    \expected{\expected{Y'X}{\set{G}}}=\expected{Y'X}&=\expected{Y'\expected{X}{\set{G}}}
\end{align*}
\end{proof}

\begin{example}
  \item Niewiele mający z tym co przed chwilą było. Niech $\Omega=[0,1]$, $\set{F}=Bor([0,1])$ i $\prob=\lambda$. Chcemy rozważyć $t\in(0,1)$ oraz $\set{G}=\sigma(Bor([0,t])$.

    Dla całkowalnej zmiennej losowej $X$ szukamy $\expected{X}{\set{G}}$.

    Dowolny $G\in\set{G}$ ma postać $G=A\cup B$, gdzie $A\in Bor([0, t])$ i $B\in \{(t, 1],\emptyset\}$. W takim razie, jeśli $Y\in \set{G}$, to $Y$ jest stała na $(t, 1]$. Czyli żeby $Y=\expected{X}{\set{G}}$ to zapewne będzie postaci:
    $$Y(\omega)=X(\omega)\mathds{1}_{[0, t]}(\omega)+c\mathds{1}_{(t, 1]}(\omega)$$
      gdzie $c$ jest pewną stałą.

      Musimy sprawdzić, czy (i kiedy) $\expected{X\mathds{1}_G}=\expected{Y\mathds{1}_{G}}$. Widać od razu, że dla $G=A\cup B$ jak wyżej, mamy
      $$X\mathds{1}_A=Y\mathds{1}_A,$$
      czyli $\expected{X\mathds{1}_A}=\expected{Y\mathds{1}_A}$. Zostaje nam uzgodnić część $B$ kiedy jest on niepusty:
      $$\expected{X\mathds{1}_B}=\int_t^1X(s)ds$$
      $$\expected{Y\mathds{1}_B}=c(1-t),$$
      czyli $c$ musi być średnią $X$ na przedziale $(t, 1]$:
      $$c=\frac{1}{1-t}\int_t^1X(s)ds.$$
\end{example}

\subsection{Regularne rozkłady warunkowe}

Dla zmiennej losowej $Y$ i całkowalnej zmiennej losowe $X$ napis
$$\expected{X}{Y}:=\expected{X}{\sigma(Y)}$$
będzie wwo $X$ względem $\sigma$-ciała generowanego przez $Y$.


\textbf{Zadanie dla dociekliwych:}

Jeśli $\prob{Y=y}>0$ dla $y\in\R$, to biorąc $\omega\in\{Y=y\}$ dostajemy:
$$\expected{X}{Y}(\omega)=\expected{X}{Y=y}=\frac{1}{\prob{Y=y}}\expected{X\mathds{1}_{\{Y=y\}}}$$

\begin{definition}[wwo $X$ pod warunkiem $\{Y=y\}$]
 Po pierwsze zauważamy, że istnieje funkcja $h:\R\to\R$ taka, że $\expected{X}{Y}=h(Y)$. ($\star$)

  Niech $X$ i $Y$ będą dowolnymi zmiennymi losowymi, przy czym od $X$ wymagamy całkowalności. Dla $y\in\R$ warunkową wartość oczekiwaną $X$ pod warunkiem $\{Y=y\}$ definiujemy przez 
  $$\expected{X}{Y=y}=h(y)$$
  gdzie $h$ spełnia warunek ($\star$).
\end{definition}

Podobnie definiujemy prawdopodobieństwo zbioru $A$ pod warunkiem $\{Y=y\}$:
$$\prob{A}{Y=y}=\expected{\mathds{1}_A}{Y=y}$$

\begin{example}
\item Jeżeli $X$ i $Z$ są niezależne, to chcemy zapytać o
  $$\prob{X+Z\in B}{X=x}\overset{?}{=}\prob{Z+x\in B}=\mu_Z(B-x)$$

  Wysławiając tę wartość w terminach całego $X$:
  \begin{align*}
    \prob{X+Z\in B}{X}&=\expected{\mathds{1}_{X+Z\in B}}{X}\overset{\star}{=}\expected{\phi(X, Z)}{X}=\Phi(X)
  \end{align*}
  w $\star$ definiujemy: $\phi(x,z)=\mathds{1}_{x+z\in B}$. Dla ustalonego $x$ mamy więc:
  $$h(x)=\expected{\phi(x, Z)}=\expected{\mathds{1}_{x+Z\in B}}=\prob{Z+x\in B}$$
\item Niech wektor losowy $(X, Y)$ ma gęstość łączną $f(x,y)$. Wówczas
  $$\prob{X\in B}{Y=y}=\int_Bf_{X|Y}(x,y)dx,$$
  gdzie 
  $$f_{X|Y}(x,y)=\frac{f(x,y)}{\int f(t,y)dt}.$$
\item Rozważmy $\prob{E_1\in\bullet}{E_1+E_2=y}$ dla $E_1,E_2$ niezależnych o rozkładzie $Exp(1)$.

  Przyłożymy do tego przypadku wzór z przykładu 2. Wektor losowy $(E_1, E_1+E_2)$ ma rozkład losowy o łącznej gęstości $f(x, y)=e^{-x}e^{-(y-x)}\mathds{1}_{y\geq x\geq0}$. 
  $$\int f(s, y)ds=\int_0^ye^{-y}ds=y,$$
  czyli 
  $$f_{E_1|E_1+E_2}(x, y)=\frac{1}{y}\mathds{1}_{y\geq x\geq 0}$$
  co daje rozkład jednostajny:
  $$\prob{E_1\in\bullet}{E_1+E_2=y}=U[0, y](\bullet)$$
\end{example}
\bigskip

Można zadać sobie pytanie, czy
$$\prob{A}{Y=y}=\expected{\mathds{1}_A}{Y=y}$$
zawsze zadaje miarę probabilistyczną? Okazuje się, że tak faktycznie jest.

\begin{definition}[regularny rozkład warunkowy]
  Niech $X$ będzie zmienną losową, a $\set{G}\subseteq\set{F}$ będzie $\sigma$-ciałem. Funkcja 
  $$\kappa_{X,\set{G}}:\Omega\times Bor(\R)\to [0,1]$$
  nazywa się \buff{regularnym rozkładem warunkowym} [rrw] $X$ pod warunkiem $\set{G}$, jeżeli
  \begin{itemize}
    \item[(R1)] Dla każdego $B\in Bor(\R)$ zmienna losowa $\kappa_{X,\set{G}}(\bullet, B)$ jest $\set{G}$-mierzalna
    \item[(R2)] Dla każdej $\omega\in\Omega$ $\kappa_{X,\set{G}}(\omega,\bullet)$ jest rozkładem prawdopodobieństwa na prostej.
    \item[(R3)] Dla każdego $B\in Bor(\R)$ i $\prob$-p.w. $\omega\in\Omega$
      $$\prob{X\in B}{\set{G}}(\omega)=\kappa_{X,\set{G}}(\omega, B)$$
  \end{itemize}
\end{definition}

\begin{theorem}[rrw istnieje]
  Dla każdego $X$ i dla każdego $\set{G}$ istnieje rrw $X$ pod warunkiem $\set{G}$
\end{theorem}

\begin{proof}
  W notatkach
\end{proof}

\begin{fact}\label{fakt 3.5}
  Dla funkcji $f:\R\to\R$ i zmiennej losowej $X$ takiej, że $\expected{|f(X)|}<\infty$ mamy
  $$\expected{f(X)}{\set{G}}(\omega)=\int_{\R} f(x)\kappa_{X,\set{G}}(\omega,dx)$$
\end{fact}

Pisząc to mówię "weź $f(x)$ i weź miarę $\kappa$ w punkcie $\omega$ i scałkuj $\kappa$".

\begin{proof}
  Ćwiczenia.

  Będziemy przechodzić od najprostszych możliwych funkcji $f$ do coraz to bardziej skomplikowanych konstrukcji.

  W pierwszych kroku niech $f(x)=\mathds{1}_B(x)$ dla $B\in Bor(\R)$. Wówczas 
  \begin{align*}
  \expected{f(X)}{\set{G}}(\omega)&=\expected{\mathds{1}_B(X)}{\set{G}}(\omega)=\expected{\mathds{1}_{\{X\in B\}}}{\set{G}}(\omega)=\\ 
                                  &=\prob{X\in B}{\set{G}}(\omega)=\kappa_{X,\set{G}}(\omega, B)=\\ 
                                  &=\int_B\kappa_{X,\set{G}}(\omega, dx)=\int_{\R} \mathds{1}_B(x)\kappa_{X,\set{G}}(\omega, dx)
  \end{align*} 

  Dalej, niech $f(x)=\sum a_i\mathds{1}_{A_i}(x)$. Wtedy mamy
  \begin{align*}
    \expected{f(X)}{\set{G}}(\omega)&=\expected{\sum a_i\mathds{1}_{A_i}(X)}{\set{G}}(\omega)=\sum a_i\expected{\mathds{1}_{A_i}(X)}{\set{G}}(\omega)=\\ 
                                    &=\sum a_i\int_{\R}\mathds{1}_{A_i}(x)\kappa_{X,\set{G}}(\omega, dx)=\int_{\R}\sum a_i\mathds{1}_{A_i}(x)\kappa_{X,\set{G}}(\omega, dx)=\\ 
                                    &=\int_{\R}f(x)\kappa_{X,\set{G}}(\omega, dx)
  \end{align*}

  Teraz niech $f(x)=\lim s_i(x)$ dla $0\leq s_1\leq s_2\leq...\leq f$ dla prostych funkcji $s_i$ jak z poprzednich podpunktów. Zauważmy, że mamy tutaj predyspozycje do skorzystania z twierdzenia o zbieżności monotonicznej.
  \begin{align*}
    \expected{f(X)}{\set{G}}(\omega)&=\expected{\lim s_i(X)}{\set{G}}(\omega)=\lim\expected{s_i(X)}{\set{G}}(\omega)=\\ 
                            &=\lim \int_{\R} s_i(x)\kappa_{X,\set{G}}(\omega, dx)=\int_{\R}\lim s_i(x)\kappa_{X,\set{G}}(\omega, dx)=\\ 
                            &=\int_{\R}f(x)\kappa_{X,\set{G}}(\omega, dx)
  \end{align*}

  Ostatni krok w dowodzie to $f=f^+-f^-$ i korzysta się tutaj już tylko z poprzednich podpunktów oraz liniowości wwo:
  \begin{align*}
    \expected{f(X)}{\set{G}}(\omega)&=\expected{f^+(X)-f^-(X)}{\set{G}}(\omega)=\expected{f^+(X)}{\set{G}}(\omega)-\expected{f^-(X)}{\set{G}}(\omega)=\\ 
                            &=\int_{\R}f^+(x)\kappa_{X,\set{G}}(\omega, dx)-\int_{\R}f^-(x)\kappa_{X,\set{G}}(\omega, dx)=\\ 
                            &=\int_{\R}(f^+(x)-f^-(x))\kappa_{X,\set{G}}(\omega, dx)=\int_{\R}f(x)\kappa_{X,\set{G}}(\omega, dx)
  \end{align*}
\end{proof}

Jeżeli $G=\sigma(Z)$, to pojęcie rrw troszkę się upraszcza (z naciskiem na trochę):

\begin{definition}
  Dla zmiennych losowych $X,Y$ funkcję $\kappa_{X,Y}:\R\times Bor(\R)\to[0,1]$ nazywamy rrw $X$ pod warunkiem $Y$, jeżeli:
  \begin{itemize}
    \item[(P1)] Dla każdego $B\in Bor(\R)$ funkcja $\kappa_{X,Y}(\bullet, B)$ jest borelowska
    \item[(P2)] Dla każdego $y\in\R$, $\kappa_{X,Y}(y,\bullet)$ jest rozkładem prawdopodobieństwa na prostej.
    \item[(P3)] $\prob{X\in B}{Y=y}=\kappa_{X,Y}(y, B)$
  \end{itemize}
\end{definition}
