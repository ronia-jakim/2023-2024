\subsection{Zadania}

\begin{problem}
Niech $\set{G}\subseteq\set{F}$ będzie $\sigma$-ciałem. Rozważmy $\set{G}$-mierzalną zmienną losową $X$ oraz niezależną od $\set{G}$ zmienną losową $Y$. Załóżmy, że $\psi:\R^2\to\R$ jest taką funkcją mierzalną, że $\expected{|\psi(X, Y)|}<\infty$. Pokaż, że 
$$\expected{\psi(X, Y)}{\set{G}}=\Psi(X)\quad \Psi(x)=\expected{\psi(x, Y)}$$
\end{problem}

\begin{solution}
  Patrz dowód twierdzenia \ref{2.6 twierdzenie}.
\end{solution}

\begin{problem}
  Niech  $(X, Y)$ będzie dwuwymiarowym  wektorem losowych o rozkładzie jednostajnym na kwadracie o wierzchołkach $(-1, 0), (0, -1), (1, 0), (0, 1)$. Oblicz $\prob{X>\frac{1}{2}}{\set{Y}}$.
\end{problem}

\begin{solution}
  %Z definicji prawdopodobieństwa warunkowego mamy
  %$$\prob{X>\frac{1}{2}}{\set{Y}}=\expected{\mathds{1}_{\{X>1/2\}} }{\set{Y}}$$
  Zaczniemy od znalezienia gęstości rozkładu $Y$.


  Oczywiście, gęstość rozkładu wektora $(X, Y)=\frac{1}{2}$ gdyż kwadrat ma pole $2$. W takim razie, gęstość zmiennej $Y$ to 
  \begin{align*}
    f_Y(y)&=\begin{cases}\int_{y-1}^{1-y}\frac{1}{2}dx  & y \geq 0\\ \int^{1+y}_{-y-1}\frac{1}{2}dx & y<0\end{cases}=\\ 
          &= \begin{cases}1-y & y\geq 0\\ 1+y&y<0\end{cases}=1-|y|
  \end{align*}
  %W normalnych okolicznościach, prawdopodobieństwo, że $X>\frac{1}{2}$ wynosi
  %$$\prob{X>\frac{1}{2}}=\int_{1/2}^1(1-x)dx=\frac{1}{8}$$

  Skorzystamy z zadania 4 z listy 1, gdzie pokazaliśmy, że 
  $$\expected{h(X)}{Y}=\int_{\R}h(x)f_{X|Y}(x, Y)dx$$
  gdzie 
  $$f_{X|Y}(x, y)=\begin{cases}\frac{f(x, y)}{f_Y(y)}&f_Y(y)\neq 0\\0&wpp.\end{cases}$$
  W tym zadaniu chcemy wyliczyć
  \begin{align*}
    \prob{2X>1}{Y}&=\expected{\mathds{1}_{\{2X>1\}}}{Y}=\int_{1/2}^1\mathds{1}_{\square}(x, Y)\frac{1}{2-2|Y|}dx=\\ 
                  &=\int_{1/2}^{1-|Y|} \frac{ \mathds{1}_{[-1/2,1/2]}(Y) }{2-2|Y|}dx=\mathds{1}_{[-1/2, 1/2]}(Y)\frac{1/2-|Y|}{2-2|Y|}
    %\int_{\R}\mathds{1}_{\{X>\frac{1}{2}\}}(x)\frac{f(x, y)}{f_Y(y)}dx=\\ 
                  %&=\int_{1/2}^1\frac{1/2\mathds{1}_{[-1,1]}}{1-|Y|}dx=\int_{1/2}^1\frac{\mathds{1}_{[-1,1]}
  \end{align*}


  %\begin{center}\begin{tikzpicture}
  %  \filldraw[color=orange!30, fill=yellow!20, very thick] (-1.5, 0)--(0, 1.5)--(1.5, 0)--(0, -1.5)--cycle;
  %  \filldraw[very thick, color=orange!50, fill=yellow!40] (1.5, 0)--(0.75, 0.75)--(0.75, -0.75)--cycle;
  %  \draw[color=blue!50, ultra thick] (-1.5, 0)--(1.5, 0)--(1.25, 0.25)--(-1.25, 0.25)--cycle;
  %  \draw[color=blue!50, ultra thick] (1, 0.5)--(-1, 0.5)--(-0.8, 0.7)--(0.8, 0.7)--cycle;
  %  \draw[color=blue!50, ultra thick] (-1.3, -0.2)--(1.3, -0.2)--(0.8, -0.7)--(-0.8, -0.7)--cycle;
  %  \draw[->] (0, -2)--(0, 2.3);
  %  \draw[->] (-2, 0)--(2.3, 0);
  %\end{tikzpicture}\end{center}

  %Niech $C\in\sigma(Y)$ takie, że $C=\{Y\in B\}$ dla $B\in Bor(\R)$ (niebieskie kształty na obrazku). Wtenczas
  %\begin{align*}
  %  \expected{\prob{2X>1}{Y}\mathds{1}_C}&=\prob{\{2X>1\}\cap C}=\\ 
  %                                       &=\prob{\{2X>1\;i\;Y\in B\}}=\\ 
  %                                       &=\int_{B\cap [0, 1/2]}\int_{1/2}^{1-y}\frac{1}{2}dxdy+\int_{B\cap[-1/2, 0)}\int_{1/2}^{1+y}\frac{1}{2}dxdy=\\ 
  %                                       &=\frac{1}{2}\int_{B\cap [0, 1/2]}\left[\frac{1}{2}-y\right]dy+\frac{1}{2}\int_{B\cap[-1/2, 0)}\left[{\frac{1}{2}+y}\right]dy
  %\end{align*}

  %Czyli
  %$$\prob{2X>1}{Y}=\frac{1}{2}\begin{cases}\frac{1}{2}-Y & Y\in[0, \frac{1}{2}]\\ \frac{1}{2}+Y & Y\in[-\frac{1}{2}, 0)\\ 0 & wpp.\end{cases}=h(Y),$$
  %bo dla $C$ jak wyżej
  %\begin{align*}
  %  \expected{h(Y)\mathds{1}_C}&=\expected{h(Y)\mathds{1}_{B\cap [0, 1/2]}}+\expected{h(Y)\mathds{1}_{B\cap[-1/2, 0)}}=\\ 
  %                             &=\frac{1}{2}\int_{B\cap [0, 1/2]}({1/2-y}) dy + \frac{1}{2}\int_{B\cap [-1, 0)}({1/2+y})dy
  %\end{align*}

  %[najmniej pewny fragment rozwiązania ahead]

  %Próbując interpretować to geometrycznie, widać, że $2X>1$ tylko jeśli $Y\in[-1/2, 1/2]$. Dalej, jeśli ustalimy sobie jakiegoś dowolnego $Y$ z tego przedziału, to odległość od bliższego ogranicznika tego obszaru wynosi $\frac{1}{2}-Y$, jeśli $Y\geq 0$. Mnożąc to przez $2$ dostalibyśmy pole całego takiego paska: $[-1, 1]\times [Y, 1/2]$. Natomiast mnożąc przez $\frac{1}{2}$ dostajemy pole paska $[1/2, 1]\times[Y, 1/2]$.
  %%stosunek długości kreski równoległej do $OX$ na wysokości tego $Y$ zachaczającej o $2X>1$ jest 1/4 długości całej kreski:
  %\begin{center}\begin{tikzpicture}
  %  \filldraw[color=orange!30, fill=yellow!20, very thick] (-1.5, 0)--(0, 1.5)--(1.5, 0)--(0, -1.5)--cycle;
  %  \filldraw[very thick, color=orange!50, fill=yellow!40] (1.5, 0)--(0.75, 0.75)--(0.75, -0.75)--cycle;
  %  \draw[->] (0, -2)--(0, 2.3);
  %  \draw[->] (-2, 0)--(2.3, 0);
  %  \draw[dashed, very thick, blue!40] (1, 0.75)--(-1, 0.75);
  %  \draw[dashed, very thick, blue!40] (1, -0.75)--(-1, -0.75);
  %  \draw[red, thick, dashed] (0.75, 0.25)--(-1.25, 0.25);
  %  \draw[red, thick] (1.25, 0.25)--(0.75, 0.25);
  %\end{tikzpicture}\end{center}
  \end{solution}

\begin{problem}
  Niech $\{X_n\}$ będzie ciągiem niezależnych zmiennych losowych o takim samym rozkładzie z wartością oczekiwaną $m$. Niech $N$ będzie dyskretną zmienną losową o wartościach w $\N$ niezależną od ciągu $\{X_n\}$ z wartością oczekiwaną $M$. Zdefiniujmy $S_n=\sum_{k=1}^nx_k$. Znajdź
  $$\expected{S_N}{N}\quad oraz\quad \expected{S_N}$$
\end{problem}

\begin{solution}
  Możemy od razu zacząć od tezy, że
  $$\expected{S_N}{N}=N\cdot m$$
  ale spróbujemy rozwiązać to w bardziej metodyczny sposób.

  Niech $G\in\sigma(N)$, czyli $G=\{N\in C\}$ dla $C\in Bor(\R)$.
  \begin{align*}
    \expected{\expected{S_N}{N}\mathds{1}_G}&=\expected{S_n\mathds{1}_G}=\\ 
                                            &=\expected{\sum_{k=1}^NX_k\mathds{1}_{\{N\in C\}}}=\\ 
                                            &=\sum_{n\in C}\expected{\sum_{k=1}^nX_k}\prob{N=n}=\\ 
                                            &=\sum_{n\in C}\sum_{k=1}^n\prob{N=n}\expected{\sum_{k=1}^nX_k}=\\ 
                                            &=m\sum_{n\in C}n\cdot \prob{N=n}=\\ 
                                            &=m\cdot\expected{N\mathds{1}_G}=\expected{N\cdot m\mathds{1}_G}
  \end{align*}
  Czyli warunek (W2) jest spełniony przez $Nm$, a warunek $\sigma(N)$-mierzlaności jest spełniony przez fakt, że $N$ jest $\sigma(N)$-mierzalne.

  Korzystając z 1. własności wwo (\ref{o arytmetyce wwo}) wiemy, że
  $$\expected{S_N}=\expected{\expected{S_N}{N}}=\expected{N\cdot m}=m\cdot M$$
\end{solution}

\begin{problem}
  Zmienne losowe $X$ i $Y$ są niezależne i mają rozkład $Exp(1)$. 
  \begin{enumerate}[label=(\alph*)]
    \item Oblicz $\expected{X+Y}{X}$
    \item Oblicz $\expected{X}{X+Y}$
  \end{enumerate}
\end{problem}

\begin{solution}$ $

  \begin{enumerate}[label=(\alph*)]
    \item Zacznijmy od szybkiego przypomnienia, że jeśli $X$ i $Y$ są niezależne, to $Y$ jest niezależne od $\sigma(X)$. Dla $A, B\in Bor(\R)$ i $G=\{X\in B\}\in\sigma(X)$ mamy
  \begin{align*}
    \prob{Y\in A}{G}&=\prob{Y\in A}{X\in B}=\\ 
                    &=\prob{Y\in A}\prob{X\in B}=\prob{Y\in A}\prob{G}
  \end{align*}

  Czyli wracając do treści zadania
  \begin{align*}
    \expected{X+Y}{X}&=\expected{X}{X}+\expected{Y}{X}=X+\expected{Y}=X+1
  \end{align*}
  gdyż $X$ jest mierzalne względem $\sigma(X)$, więc $\expected{X}{X}=X$, a z drugiej strony ponieważ $Y$ jest niezależne od $\sigma(X)$, to $\expected{Y}{X}=\expected{Y}$.

  \item Zaczniemy od obserwacji, że
    $$\expected{X}{X+Y}=\expected{Y}{X+Y}$$
    ponieważ
    \begin{align*}
      \expected{ \expected{X}{X+Y} \mathds{1}_{\{X+Y\in C\}} } &=\expected{ X\mathds{1}_{\{X+Y\in C\}} }=\\ 
                                                               &=\expected{ Y\mathds{1}_{ \{X+Y\in C\} } }=\\ 
                                                               &=\expected{ \expected{Y}{X+Y}\mathds{1}_{\{X+Y\in C\}} }
    \end{align*}

    W takim razie 
    \begin{align*}
      \expected{X}{X+Y}&=\frac{1}{2}(\expected{X}{X+Y}+\expected{Y}{X+Y})=\frac{1}{2}\expected{X+Y}{X+Y}=\frac{1}{2}(X+Y)
    \end{align*}
\end{enumerate}
\end{solution}

\begin{problem}
  Pokaż, że jeśli $X$ i $Y$ są zmiennymi losowymi takimi, że $X$ oraz $XY$ są całkowalne oraz $Y$ jest zmienną losową mierzalną względem $\set{G}$, to
  $$\expected{XY}{\set{G}}=Y\expected{X}{\set{G}}$$
\end{problem}

\begin{solution}
  Patrz dowód twierdzenia \ref{o arytmetyce wwo}.
\end{solution}

\begin{problem}
  Niech $X$ będzie całkowalną zmienną losową. Niech $\set{C}\subseteq\set{G}$ będzie $\pi$-układem generującym $\sigma$-ciało $\sigma(\set{C})=\set{G}\subseteq\set{F}$.
\end{problem}

\begin{solution}
  Zadanie sprowadza się do skorzystania z lematu o $\pi-\lambda$ układach i sprawdzeniu czy zbiór
  $$D=\{A\;:\;\expected{X\mathds{1}_A}=\expected{Z\mathds{1}_A}\}$$
  zawierający $\pi$-układ $\set{C}$ jest $\lambda$-układem. Wówczas $D$ samo w sobie będzie $\sigma$-układem, w szczególności zawierającym ciało $\set{G}$.

  \begin{enumerate}
    \item $\Omega\in D$ bo aby $\set{C}$ generowało $\sigma$-ciało, to musi zawierać $\Omega$.
    \item $A\subseteq B\in D\implies B\setminus A\in D$

      Niech $A\subseteq B\in D$, wówczas $\mathds{1}_{B\setminus A}=\mathds{1}_B-\mathds{1}_A$. Daje to:
      \begin{align*}
        \expected{X\mathds{1}_{B\setminus A}}&=\expected{X(\mathds{1}_B-\mathds{1}_A)}=\\ 
                                             &=\expected{X\mathds{1}_B}-\expected{X\mathds{1}_A}=\\ 
                                             &=\expected{Z\mathds{1}_B}-\expected{Z\mathds{1}_A}=\\ 
                                             &=\expected{Z(\mathds{1}_B-\mathds{1}_A)}=\\ 
                                             &=\expected{Z\mathds{1}_{B\setminus A}}
      \end{align*}
      czyli z $B\setminus A$ zaspokaja warunek należenia do $D$.
    \item $A_1\subseteq A_2\subseteq...\in D\implies\bigcup A_i\in D$
    
      Dla pokazania tego warunku będziemy korzystać z twierdzenia o zbieżności monotonicznej. Zauważmy, że dla każdego $n\in\N$ zachodzi $X\mathds{1}_{\bigcup A_i}\geq X\mathds{1}_{A_n}$ oraz $X\mathds{1}_{A_1}\leq X\mathds{1}_{A_2}\leq...$.
      \begin{align*}
        \expected{X\mathds{1}_{\bigcup A_i}}&=\expected{X\lim\mathds{1}_{A_i}}=\\ 
                                            &=\lim\expected{X\mathds{1}_{A_i}}=\\ 
                                            &=\lim\expected{Z\mathds{1}_{A_i}}=\\ 
                                            &+\expected{Z\lim\mathds{1}_{A_i}}=\expected{Z\mathds{1}_{\bigcup A_i}}
      \end{align*}
  \end{enumerate}
  Czyli tak długo jak $Z$ jest $\set{G}$-mierzalne, to jest ono wwo $X$ pod warunkiem że $\set{G}$.
\end{solution}

\begin{problem}
  Niech $\set{G}, \set{D}\subseteq\set{F}$ będą niezależnymi $\sigma$-ciałami. Niech $X$ będzie całkowalną zmienną losową.
  \begin{enumerate}[label=(\alph*)]
    \item Załóżmy, że $X$ jest niezależna od $\sigma$-ciala $\set{D}$. Czy prawdą jest, że 
      $$\expected{X}{\sigma(\set{G}\cup\set{D})}=\expected{X}{\set{G}}?\quad (\star)$$
    \item Pokaż, że jeżeli $\set{D}$ jest niezależne od $\sigma(\sigma(X)\cup\set{G})$, to $(\star)$ zachodzi.
  \end{enumerate}
\end{problem}
